# -*- coding: utf-8 -*-
"""KW_MMDS - MidTerm HW 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xaNQ-9A2PPm6VQx73-dkisKZr-6OUe95

# KW_MMDS - MidTerm HW 2
## Cross-selling

### Setup

Let's setup Spark on your Colab environment.  Run the cell below!
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.

**Make sure to follow the interactive instructions.**
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1H6HXH1ruOM_6bQ4uzbpW2una-ILvjFQc'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('browsing.txt')

"""If you executed the cells above, you should be able to see the dataset we will need for this Colab under the "Files" tab on the left panel."""

# Commented out IPython magic to ensure Python compatibility.
# Let's import the libraries we will need
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf

"""Let's initialize the Spark context."""

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

"""### Basics

Association Rules are frequently used for Market Basket Analysis (MBA) by retailers to
understand the purchase behavior of their customers. This information can be then used for
many different purposes such as cross-selling and up-selling of products, sales promotions,
loyalty programs, store design, discount plans and many others.


<b>Evaluation of item sets:</b> Once you have found the frequent itemsets of a dataset, you need
to choose a subset of them as your recommendations. Commonly used metrics for measuring
significance and interest for selecting rules for recommendations are:

1. <b>Confidence</b> (denoted as $\mathrm{conf}(A \to B)$): Confidence is defined as the probability of
occurrence of $B$ in the basket if the basket already contains $A$:<br>
$\mathrm{conf}(A \to B)$ = $\mathrm{Pr}(B|A)$;<br>
where $\mathrm{Pr}(B|A)$ is the conditional probability of finding item set $B$ given that item set
$A$ is present.
2. <b>Lift</b> (denoted as $\mathrm{lift}(A \to B)$): Lift measures how much more "$A$ and $B$ occur together"
than "what would be expected if $A$ and $B$ were statistically independent":<br>
$\mathrm{lift}(A \to B) = \frac{\mathrm{conf}(A \to B)}{S(B)}$,<br>
where $S(B) = \frac{\mathrm{Support}(B)}{N}$ and $N$ = total number of transactions (baskets).
3. <b>Conviction</b> (denoted as $\mathrm{conv}(A \to B)$): Conviction compares the "probability that
$A$ appears without $B$ if they were independent" with the "actual frequency of the
appearance of $A$ without $B$":
$\mathrm{conv}(A \to B) = \frac{1 − S(B)}{1 − \mathrm{conf}(A \to B)}$



#### Code
If you run successfully the setup stage, you are ready to work on the *browsing.txt* file. Write a Spark program that performs fast Frequent Pattern Mining with the FP-Growth algorithm. 

Problem. Identify pairs of items $(X, Y)$ such that the support of $\{X,Y\}$ is at least 100. For all such pairs, generate association rules $X \to Y$, sort the rules in decreasing order of <i>confidence</i> scores and list the top 5 rules.
"""

# YOUR CODE HERE
# 한 줄씩 읽기
RDDs = sc.textFile('browsing.txt')
RDDs.take(5)

# 마지막 공백 제거
RDDs = RDDs.map(lambda line: line.strip())
RDDs.take(5)

# 한 줄을 공백으로 구분하여 리스트로 변환
RDDs = RDDs.map(lambda lst:list(lst.split(' ')))
RDDs.take(5)

# 각 리스트에서 집합을 사용해 중복되는 값 제거
RDDs = RDDs.map(lambda lst: list(set(lst)))
RDDs.take(5)

# DataFrame으로 변환하기 위해 tuple로 변환
RDDs = RDDs.map(lambda lst: (lst,))
RDDs.take(5)

# DataFrame으로 변환
df = spark.createDataFrame(RDDs, ['items'])
df.show(truncate=False)

# 모델 생성
from pyspark.ml.fpm import FPGrowth
#100
fpGrowth = FPGrowth(itemsCol="items", minSupport=100/df.count(),minConfidence=0.1) 
model = fpGrowth.fit(df)


# 연관 법칙
rules = model.associationRules
rules.show()

# antecedent 한개짜리 법칙 찾기
singletons = rules.where(size(col('antecedent')) == 2)
singletons.show(5)

"""5. [10 pts] Identify item triples $(X, Y, Z)$ such that the support of $\{X, Y, Z\}$ is at least 100. For all such triples, generate association rules $(X,Y) \to Z$, sort the rules in decreasing order of <i>confidence</i> scores and list the top 20 rules."""

# YOUR CODE HERE


# 한 줄씩 읽기
RDDs = sc.textFile('browsing.txt')
RDDs.take(5)

# 마지막 공백 제거
RDDs = RDDs.map(lambda line: line.strip())
RDDs.take(5)

# 한 줄을 공백으로 구분하여 리스트로 변환
RDDs = RDDs.map(lambda lst:list(lst.split(' ')))
RDDs.take(5)

# 각 리스트에서 집합을 사용해 중복되는 값 제거
RDDs = RDDs.map(lambda lst: list(set(lst)))
RDDs.take(5)

# DataFrame으로 변환하기 위해 tuple로 변환
RDDs = RDDs.map(lambda lst: (lst,))
RDDs.take(5)

# DataFrame으로 변환
df = spark.createDataFrame(RDDs, ['items'])
df.show(truncate=False)

# 모델 생성
from pyspark.ml.fpm import FPGrowth
#100
fpGrowth = FPGrowth(itemsCol="items", minSupport=100/df.count(),minConfidence=0.1) 
model = fpGrowth.fit(df)


# 연관 법칙
rules = model.associationRules
rules.show()

# antecedent 두개짜리 법칙 찾기
singletons = rules.where(size(col('antecedent')) == 2)
singletons.show()
